# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run. Models are created with PythonSDK and AutoML and then compared.

## Summary
**The given dataset contains marketing data of a bank and the sample dataset is provided by Azure at:
https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv

Best Performing Model: In this given dataset, 


**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

## Scikit-learn Pipeline
(i) Data is loaded as a very first step, which is imported from TabularDatasetFactory. (ii) Data cleaning: Hot encoding 
was used as columns have categorical data.  (iii)Data split was used as 0.7:0.3 for traning and testing datasets respectively
(iv)Hyperparameters were chosen with Random Sampling and regression models were used for traning with hyperparameters (C, max_iter)
(v) once the experiment is completed, BanditPolicy was used for early termination. This was we can save further use of resources by
stopping the hyperparameter run. (vi)  using hyperparametrs, the best model was observed and then saved.

aPipeline was ran several times, with Hyperdrive configuration to improve our Accuracy of the model and once
satisfied we register our model for future use. In this case the best model was generated using this hyperparameters
**(C = (0, 1), max_iter = randint(100')** and give us an  **Accuracy of 0.914**

**What are the benefits of the parameter sampler you chose?**
After defining the hyperparametet to sweep over, we chose **C** and **max_iter** parameters with random sampling **RandomParameterSampling** 
to try different possible configuration with discrete C and max_iter

**What are the benefits of the early stopping policy you chose?**
We then define our termination Policy for every run using **BanditPolicy** based on a slack factor  of 0.1 equal to 
This helps to reduce the number of poorly performing runs and hence the cost.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
After cleaning the data by one-hot encoding, traninging ans testing set were defined. In the AutoML configuration, the primary metric was defined as 
"Accuracy". Then experiment was submitted with submit method and Best Model was found, which was registered for future use.  the best model was generated using **VotingEnsemble Algorithm** which involves summing the predictions made by multiple other classification models and give us an  **Accuracy of 0.92**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
Based on primary metric "Accuracy" both models were compared and Auto ML is gives better performance, as ite tests more alogorithms and hyperparameters compared to 
Scikit-learn pipeline.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
More algorithms can be added to Scikit-learn process to test other configuration  and tune hyperparameters.


## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
Once finished we delete the compute instance and the compute cluster used during this project to not incur any charges.

